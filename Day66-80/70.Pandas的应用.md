## Pandas 的应用-1

Pandas 是 Wes McKinney 在 2008 年开发的一个强大的**分析结构化数据**的工具集。Pandas 以 NumPy 为基础（数据表示和运算），提供了用于数据处理的函数和方法，对数据分析和数据挖掘提供了很好的支持；同时 Pandas 还可跟数据可视化工具 Matplotlib 很好的整合在一起，非常轻松愉快的实现数据的可视化展示。

Pandas 核心的数据类型是`Series`（数据系列）、`DataFrame`（数据表/数据框），分别用于处理一维和二维的数据，除此之外还有一个名为`Index`的类型及其子类型，它为`Series`和`DataFrame`提供了索引功能。日常工作中以`DataFrame`使用最为广泛，因为二维的数据本质就是一个有行有列的表格（想一想 Excel 电子表格和关系型数据库中的二维表）。上述这些类型都提供了大量的处理数据的方法，数据分析师可以此为基础实现对数据的各种常规处理。

### Series 的应用

Pandas 库中的`Series`对象可用来表示一维数据结构，跟数组非常类似，但是多了一些额外的功能。`Series`的内部结构包含了两个数组，其中一个用来保存数据，另一个用来保存数据的索引。

#### 创建 Series 对象

> 在执行下面的代码之前，请先导入`pandas`以及相关的库文件，具体的做法参考上一章。

##### 方法 1：通过列表或数组创建 Series 对象

代码：

```py
# data参数表示数据，index参数表示数据的索引（标签）
# 如果没有指定index属性，默认使用数字索引
ser1 = pd.Series(data=[320, 180, 300, 405], index=['一季度', '二季度', '三季度', '四季度'])
ser1
```

输出：

```
一季度    320
二季度    180
三季度    300
四季度    405
dtype: int64
```

##### 方法 2：通过字典创建 Series 对象。

代码：

```py
# 字典中的键就是数据的索引（标签），字典中的值就是数据
ser2 = pd.Series({'一季度': 320, '二季度': 180, '三季度': 300, '四季度': 405})
ser2
```

输出：

```
一季度    320
二季度    180
三季度    300
四季度    405
dtype: int64
```

#### 索引和切片

跟数组一样，Series 对象也可进行索引和切片操作，不同的是 Series 对象因为内部维护了一个保存索引的数组，所以除了可使用整数索引通过位置检索数据外，还可通过自己设置的索引标签获取对应的数据。

##### 使用整数索引

代码：

```py
print(ser2[0], ser[1], ser[2], ser[3])
ser2[0], ser2[3] = 350, 360
print(ser2)
```

输出：

```
320 180 300 405
一季度    350
二季度    180
三季度    300
四季度    360
dtype: int64
```

> 如果要使用负向索引，必须在创建`Series`对象时通过`index`属性指定非数值类型的标签。

##### 使用自定义的标签索引

代码：

```py
print(ser2['一季度'], ser2['三季度'])
ser2['一季度'] = 380
print(ser2)
```

输出：

```
350 300
一季度    380
二季度    180
三季度    300
四季度    360
dtype: int64
```

##### 切片操作

代码：

```py
print(ser2[1:3])
print(ser2['二季度':'四季度'])
```

输出：

```
二季度    180
三季度    300
dtype: int64
二季度    500
三季度    500
四季度    520
dtype: int64
```

代码：

```py
ser2[1:3] = 400, 500
ser2
```

输出：

```
一季度    380
二季度    400
三季度    500
四季度    360
dtype: int64
```

##### 花式索引

代码：

```py
print(ser2[['二季度', '四季度']])
ser2[['二季度', '四季度']] = 500, 520
print(ser2)
```

输出：

```
二季度    400
四季度    360
dtype: int64
一季度    380
二季度    500
三季度    500
四季度    520
dtype: int64
```

##### 布尔索引

代码：

```py
ser2[ser2 >= 500]
```

输出：

```
二季度    500
三季度    500
四季度    520
dtype: int64
```

####属性和方法

Series 对象的常用属性如下表所示。

| 属性                      | 说明                                    |
| ------------------------- | --------------------------------------- |
| `dtype` / `dtypes`        | 返回`Series`对象的数据类型              |
| `hasnans`                 | 判断`Series`对象中有没有空值            |
| `at` / `iat`              | 通过索引访问`Series`对象中的单个值      |
| `loc` / `iloc`            | 通过一组索引访问`Series`对象中的一组值  |
| `index`                   | 返回`Series`对象的索引                  |
| `is_monotonic`            | 判断`Series`对象中的数据是否单调        |
| `is_monotonic_increasing` | 判断`Series`对象中的数据是否单调递增    |
| `is_monotonic_decreasing` | 判断`Series`对象中的数据是否单调递减    |
| `is_unique`               | 判断`Series`对象中的数据是否独一无二    |
| `size`                    | 返回`Series`对象中元素的个数            |
| `values`                  | 以`ndarray`的方式返回`Series`对象中的值 |

`Series`对象的方法很多，通过下面的代码为大家介绍一些常用的方法。

##### 统计相关的方法

`Series`对象支持各种获取描述性统计信息的方法。

代码：

```py
# 求和
print(ser2.sum())
# 求均值
print(ser2.mean())
# 求最大
print(ser2.max())
# 求最小
print(ser2.min())
# 计数
print(ser2.count())
# 求标准差
print(ser2.std())
# 求方差
print(ser2.var())
# 求中位数
print(ser2.median())
```

`Series`对象还有一个名为`describe()`的方法，可获得上述所有的描述性统计信息

代码：

```py
ser2.describe()
```

输出：

```
count      4.000000
mean     475.000000
std       64.031242
min      380.000000
25%      470.000000
50%      500.000000
75%      505.000000
max      520.000000
dtype: float64
```

> 因为`describe()`返回的也是一个`Series`对象，所以也可用`ser2.describe()['mean']`来获取平均值。

如果`Series`对象有重复的值，可使用`unique()`方法获得去重之后的`Series`对象；可使用`nunique()`方法统计不重复值的数量；如果想要统计每个值重复的次数，可使用`value_counts()`方法，这个方法会返回一个`Series`对象，它的索引就是原来的`Series`对象中的值，而每个值出现的次数就是返回的`Series`对象中的数据，在默认情况下会按照出现次数做降序排列。

代码：

```py
ser3 = pd.Series(data=['apple', 'banana', 'apple', 'pitaya', 'apple', 'pitaya', 'durian'])
ser3.value_counts()
```

输出：

```
apple     3
pitaya    2
durian    1
banana    1
dtype: int64
```

代码：

```py
ser3.nunique()
```

输出：

```
4
```

##### 数据处理的方法

`Series`对象的`isnull()`和`notnull()`方法可用于空值的判断，代码如下所示。

代码：

```py
ser4 = pd.Series(data=[10, 20, np.NaN, 30, np.NaN])
ser4.isnull()
```

输出：

```
0    False
1    False
2     True
3    False
4     True
dtype: bool
```

代码：

```py
ser4.notnull()
```

输出：

```
0     True
1     True
2    False
3     True
4    False
dtype: bool
```

`Series`对象的`dropna()`和`fillna()`方法分别用来删除空值和填充空值，具体的用法如下所示。

代码：

```py
ser4.dropna()
```

输出：

```
0    10.0
1    20.0
3    30.0
dtype: float64
```

代码：

```py
# 将空值填充为40
ser4.fillna(value=40)
```

输出：

```
0    10.0
1    20.0
2    40.0
3    30.0
4    40.0
dtype: float64
```

代码：

```py
# backfill或bfill表示用后一个元素的值填充空值
# ffill或pad表示用前一个元素的值填充空值
ser4.fillna(method='ffill')
```

输出：

```
0    10.0
1    20.0
2    20.0
3    30.0
4    30.0
dtype: float64
```

需要提醒大家注意的是，`dropna()`和`fillna()`方法都有一个名为`inplace`的参数，它的默认值是`False`，表示删除空值或填充空值不会修改原来的`Series`对象，而是返回一个新的`Series`对象来表示删除或填充空值后的数据系列，如果将`inplace`参数的值修改为`True`，那么删除或填充空值会就地操作，直接修改原来的`Series`对象，那么方法的返回值是`None`。后面会接触到的很多方法，包括`DataFrame`对象的很多方法都会有这个参数，它们的意义跟这里是一样的。

`Series`对象的`mask()`和`where()`方法可将满足或不满足条件的值进行替换

代码：

```py
ser5 = pd.Series(range(5))
ser5.where(ser5 > 0)
```

输出：

```
0    NaN
1    1.0
2    2.0
3    3.0
4    4.0
dtype: float64
```

代码：

```py
ser5.where(ser5 > 1, 10)
```

输出：

```
0    10
1    10
2     2
3     3
4     4
dtype: int64
```

代码：

```py
ser5.mask(ser5 > 1, 10)
```

输出：

```
0     0
1     1
2    10
3    10
4    10
dtype: int64
```

`Series`对象的`duplicated()`方法可帮助找出重复的数据，而`drop_duplicates()`方法可帮删除重复数据。

代码：

```py
ser3.duplicated()
```

输出：

```
0    False
1    False
2     True
3    False
4     True
5     True
6    False
dtype: bool
```

代码：

```py
ser3.drop_duplicates()
```

输出：

```
0     apple
1    banana
3    pitaya
6    durian
dtype: object
```

`Series`对象的`apply()`和`map()`方法非常重要，它们可用于数据处理，把数据映射或转换成期望的样子，这个操作在数据分析的数据准备阶段非常重要。

代码：

```py
ser6 = pd.Series(['cat', 'dog', np.nan, 'rabbit'])
ser6
```

输出：

```
0       cat
1       dog
2       NaN
3    rabbit
dtype: object
```

代码：

```py
ser6.map({'cat': 'kitten', 'dog': 'puppy'})
```

输出：

```
0    kitten
1     puppy
2       NaN
3       NaN
dtype: object
```

代码：

```py
ser6.map('I am a {}'.format, na_action='ignore')
```

输出：

```
0       I am a cat
1       I am a dog
2              NaN
3    I am a rabbit
dtype: object
```

代码：

```py
ser7 = pd.Series([20, 21, 12],  index=['London', 'New York', 'Helsinki'])
ser7
```

输出：

```
London      20
New York    21
Helsinki    12
dtype: int64
```

代码：

```py
ser7.apply(np.square)
```

输出：

```
London      400
New York    441
Helsinki    144
dtype: int64
```

代码：

```py
ser7.apply(lambda x, value: x - value, args=(5, ))
```

输出：

```
London      15
New York    16
Helsinki     7
dtype: int64
```

##### 排序和取头部值的方法

`Series`对象的`sort_index()`和`sort_values()`方法可用于对索引和数据的排序，排序方法有一个名为`ascending`的布尔类型参数，该参数用于控制排序的结果是升序还是降序；而名为`kind`的参数则用来控制排序使用的算法，默认使用了`quicksort`，也可选择`mergesort`或`heapsort`；如果存在空值，那么可用`na_position`参数空值放在最前还是最后，默认是`last`，代码如下所示。

代码：

```py
ser8 = pd.Series(
    data=[35, 96, 12, 57, 25, 89],
index=['grape', 'banana', 'pitaya', 'apple', 'peach', 'orange']
)
# 按值从小到大排序
ser8.sort_values()
```

输出：

```
pitaya    12
peach     25
grape     35
apple     57
orange    89
banana    96
dtype: int64
```

代码：

```py
# 按索引从大到小排序
ser8.sort_index(ascending=False)
```

输出：

```
pitaya    12
peach     25
orange    89
grape     35
banana    96
apple     57
dtype: int64
```

如果要从`Series`对象中找出元素中最大或最小的“Top-N”，实际上是不需要对所有的值进行排序的，可使用`nlargest()`和`nsmallest()`方法来完成

代码：

```py
# 值最大的3个
ser8.nlargest(3)
```

输出：

```
banana    96
orange    89
apple     57
dtype: int64
```

代码：

```py
# 值最小的2个
ser8.nsmallest(2)
```

输出：

```
pitaya    12
peach     25
dtype: int64
```

#### 绘制图表

Series 对象有一个名为`plot`的方法可用来生成图表，如果选择生成折线图、饼图、柱状图等，默认会使用 Series 对象的索引作为横坐标，使用 Series 对象的数据作为纵坐标。

首先导入`matplotlib`中`pyplot`模块并进行必要的配置。

```py
import matplotlib.pyplot as plt

# 配置支持中文的非衬线字体（默认的字体无法显示中文）
plt.rcParams['font.sans-serif'] = ['SimHei', ]
# 使用指定的中文字体时需要下面的配置来避免负号无法显示
plt.rcParams['axes.unicode_minus'] = False
```

创建`Series`对象并绘制对应的柱状图。

```py
ser9 = pd.Series({'一季度': 400, '二季度': 520, '三季度': 180, '四季度': 380})
# 通过Series对象的plot方法绘图（kind='bar'表示绘制柱状图）
ser9.plot(kind='bar', color=['r', 'g', 'b', 'y'])
# x轴的坐标旋转到0度（中文水平显示）
plt.xticks(rotation=0)
# 在柱状图的柱子上绘制数字
for i in range(4):
    plt.text(i, ser9[i] + 5, ser9[i], ha='center')
# 显示图像
plt.show()
```

![](https://github.com/jackfrued/mypic/raw/master/20220619171513.png)

绘制反映每个季度占比的饼图。

```py
# autopct参数可配置在饼图上显示每块饼的占比
ser9.plot(kind='pie', autopct='%.1f%%')
# 设置y轴的标签（显示在饼图左侧的文字）
plt.ylabel('各季度占比')
plt.show()
```

![](https://github.com/jackfrued/mypic/raw/master/20220619171503.png)

## Pandas 的应用-2

### DataFrame 的应用

#### 创建 DataFrame 对象

##### 通过二维数组创建`DataFrame`对象

代码：

```py
scores = np.random.randint(60, 101, (5, 3))
courses = ['语文', '数学', '英语']
ids = [1001, 1002, 1003, 1004, 1005]
df1 = pd.DataFrame(data=scores, columns=courses, index=ids)
df1
```

输出：

```
		语文	数学	英语
1001    69    80	79
1002    71	  60	100
1003    94    81	93
1004    88	  88	67
1005    82	  66    60
```

##### 通过字典创建`DataFrame`对象

代码：

```py
scores = {
    '语文': [62, 72, 93, 88, 93],
    '数学': [95, 65, 86, 66, 87],
    '英语': [66, 75, 82, 69, 82],
}
ids = [1001, 1002, 1003, 1004, 1005]
df2 = pd.DataFrame(data=scores, index=ids)
df2
```

输出：

```
		语文	数学	英语
1001    69    80	79
1002    71	  60	100
1003    94    81	93
1004    88	  88	67
1005    82	  66    60
```

##### 读取 CSV 文件创建`DataFrame`对象

可通过`pandas` 模块的`read_csv`函数来读取 CSV 文件，`read_csv`函数的参数非常多，下面接受几个比较重要的参数。

- `sep` / `delimiter`：分隔符，默认是`,`。
- `header`：表头（列索引）的位置，默认值是`infer`，用第一行的内容作为表头（列索引）。
- `index_col`：用作行索引（标签）的列。
- `usecols`：需要加载的列，可使用序号或列名。
- `true_values` / `false_values`：哪些值被视为布尔值`True` / `False`。
- `skiprows`：通过行号、索引或函数指定需要跳过的行。
- `skipfooter`：要跳过的末尾行数。
- `nrows`：需要读取的行数。
- `na_values`：哪些值被视为空值。

代码：

```py
df3 = pd.read_csv('2018年北京积分落户数据.csv', index_col='id')
df3
```

输出：

```
     name   birthday    company       score
id
1    杨x    1972-12    北京利德xxxx	  122.59
2    纪x    1974-12    北京航天xxxx	  121.25
3    王x    1974-05	  品牌联盟xxxx    118.96
4    杨x    1975-07	  中科专利xxxx    118.21
5    张x    1974-11	  北京阿里xxxx    117.79
...  ...    ...        ...            ...
6015 孙x    1978-08	  华为海洋xxxx	  90.75
6016 刘x    1976-11	  福斯流体xxxx    90.75
6017 周x    1977-10	  赢创德固xxxx    90.75
6018 赵x	   1979-07	  澳科利耳xxxx    90.75
6019 贺x	   1981-06	  北京宝洁xxxx    90.75
6019 rows × 4 columns
```

> 如果需要上面例子中的 CSV 文件，可通过下面的百度云盘地址进行获取，数据在《从零开始学数据分析》目录中。链接：https://pan.baidu.com/s/1rQujl5RQn9R7PadB2Z5g_g，提取码：e7b4。

##### 读取 Excel 文件创建`DataFrame`对象

可通过`pandas` 模块的`read_excel`函数来读取 Exce l 文件，该函数与上面的`read_csv`非常相近，多了一个`sheet_name`参数来指定数据表的名称，但是不同于 CSV 文件，没有`sep`或`delimiter`这样的参数。下面的代码中，`read_excel`函数的`skiprows`参数是一个 Lambda 函数，通过该 Lambda 函数指定只读取 Excel 文件的表头和其中 10%的数据，跳过其他的数据。

代码：

```py
import random

df4 = pd.read_excel(
    io='小宝剑大药房2018年销售数据.xlsx',
    usecols=['购药时间', '社保卡号', '商品名称', '销售数量', '应收金额', '实收金额'],
    skiprows=lambda x: x > 0 and random.random() > 0.1
)
df4
```

> 如果需要上面例子中的 Excel 文件，可通过下面的百度云盘地址进行获取，数据在《从零开始学数据分析》目录中。链接：https://pan.baidu.com/s/1rQujl5RQn9R7PadB2Z5g_g，提取码：e7b4。

输出：

```
    购药时间			社保卡号	    商品名称    销售数量	应收金额	实收金额
0	2018-03-23 星期三	10012157328		强力xx片	 1			13.8		13.80
1	2018-07-12 星期二	108207828	    强力xx片	 1	        13.8		13.80
2	2018-01-17 星期日	13358228	    清热xx液	 1		    28.0		28.00
3	2018-07-11 星期一	10031402228		三九xx灵	 5			149.0		130.00
4	2018-01-20 星期三	10013340328		三九xx灵	 3			84.0		73.92
...	...					...				...		...			...			...
618	2018-03-05 星期六	10066059228		开博xx通	 2			56.0		49.28
619	2018-03-22 星期二	10035514928		开博xx通	 1			28.0		25.00
620	2018-04-15 星期五	1006668328	    开博xx通	 2			56.0		50.00
621	2018-04-24 星期日	10073294128		高特xx灵	 1			5.6			5.60
622	2018-04-24 星期日	10073294128		高特xx灵	 10			56.0		56.0
623 rows × 6 columns
```

##### 通过 SQL 从数据库读取数据创建`DataFrame`对象

`pandas`模块的`read_sql`函数可通过 SQL 语句从数据库中读取数据创建`DataFrame`对象，该函数的第二个参数代表了需要连接的数据库。对于 MySQL 数据库，可通过`pymysql`或`mysqlclient`来创建数据库连接，得到一个`Connection` 对象，而这个对象就是`read_sql`函数需要的第二个参数，代码如下所示。

代码：

```py
import pymysql

# 创建一个MySQL数据库的连接对象
conn = pymysql.connect(
    host='47.104.31.138', port=3306,
    user='guest', password='Guest.618',
    database='hrs', charset='utf8mb4'
)
# 通过SQL从数据库读取数据创建DataFrame
df5 = pd.read_sql('select * from tb_emp', conn, index_col='eno')
df5
```

> 执行上面的代码需要先安装`pymysql`库，如果尚未安装，可先在 Notebook 的单元格中先执行`!pip install pymysql`，再运行上面的代码。上面的代码连接的是我部署在阿里云上的 MySQL 数据库，公网 IP 地址：`47.104.31.138`，用户名：`guest`，密码：`Guest.618`，数据库：`hrs`，表名：`tb_emp`，字符集：`utf8mb4`，大家可使用这个数据库，但是不要进行恶意的访问。

输出：

```
        ename    job     mgr      sal    comm    dno
eno
1359	胡一刀   销售员	3344.0   1800   200.0   30
2056	乔峰	   分析师	 7800.0   5000   1500.0	 20
3088	李莫愁	  设计师	2056.0   3500   800.0   20
3211	张无忌	  程序员	2056.0   3200   NaN     20
3233	丘处机	  程序员	2056.0   3400	NaN     20
3244	欧阳锋	  程序员	3088.0   3200	NaN     20
3251	张翠山	  程序员	2056.0   4000	NaN     20
3344	黄蓉	   销售主管	7800.0   3000	800.0   30
3577	杨过	   会计	  5566.0   2200   NaN	  10
3588	朱九真	  会计	 5566.0   2500   NaN	 10
4466	苗人凤	  销售员	3344.0   2500	NaN     30
5234	郭靖	   出纳	  5566.0   2000   NaN	  10
5566	宋远桥	  会计师	7800.0   4000   1000.0  10
7800	张三丰	  总裁	 NaN      9000   1200.0  20
```

#### 基本属性和方法

在开始讲解`DataFrame`的属性和方法前，先从之前提到的`hrs`数据库中读取三张表的数据，创建出三个`DataFrame`对象，代码如下所示。

```py
import pymysql

conn = pymysql.connect(
    host='47.104.31.138', port=3306,
    user='guest', password='Guest.618',
    database='hrs', charset='utf8mb4'
)
dept_df = pd.read_sql('select * from tb_dept', conn, index_col='dno')
emp_df = pd.read_sql('select * from tb_emp', conn, index_col='eno')
emp2_df = pd.read_sql('select * from tb_emp2', conn, index_col='eno')
```

得到的三个`DataFrame`对象如下所示。

部门表（`dept_df`），其中`dno`是部门的编号，`dname`和`dloc`分别是部门的名称和所在地。

```
    dname  dloc
dno
10	会计部	北京
20	研发部	成都
30	销售部	重庆
40	运维部	天津
```

员工表（`emp_df`），其中`eno`是员工编号，`ename`、`job`、`mgr`、`sal`、`comm`和`dno`分别代表员工的姓名、职位、主管编号、月薪、补贴和部门编号。

```
        ename    job        mgr      sal     comm    dno
eno
1359	胡一刀    销售员	   3344.0	1800	200.0	30
2056	乔峰	    分析师	    7800.0	 5000	 1500.0	 20
3088	李莫愁	   设计师	   2056.0	3500	800.0	20
3211	张无忌	   程序员	   2056.0	3200	NaN     20
3233	丘处机	   程序员	   2056.0	3400	NaN	    20
3244	欧阳锋	   程序员	   3088.0	3200	NaN     20
3251	张翠山	   程序员	   2056.0	4000	NaN	    20
3344	黄蓉	    销售主管   7800.0	3000	800.0	30
3577	杨过	    会计	     5566.0	  2200	  NaN	  10
3588	朱九真	   会计	    5566.0	 2500	 NaN	 10
4466	苗人凤	   销售员	   3344.0	2500	NaN	    30
5234	郭靖	    出纳	     5566.0	  2000	  NaN	  10
5566	宋远桥	   会计师	   7800.0	4000	1000.0	10
7800	张三丰	   总裁	    NaN      9000	 1200.0	 20
```

> 在数据库中`mgr`和`comm`两个列的数据类型是`int`，但是因为有缺失值（空值），读取到`DataFrame`之后，列的数据类型变成了`float`，因为通常会用`float`类型的`NaN`来表示空值。

员工表（`emp2_df`），跟上面的员工表结构相同，但是保存了不同的员工数据。

```
        ename    job    mgr     sal      comm    dno
eno
9800	骆昊	   架构师	7800	30000	 5000	 20
9900	王小刀	  程序员  9800	   10000	1200	20
9700	王大锤	  程序员  9800    8000 	600	    20
```

`DataFrame`对象的属性如下表所示。

| 属性名         | 说明                                |
| -------------- | ----------------------------------- |
| `at` / `iat`   | 通过标签获取`DataFrame`中的单个值。 |
| `columns`      | `DataFrame`对象列的索引             |
| `dtypes`       | `DataFrame`对象每一列的数据类型     |
| `empty`        | `DataFrame`对象是否为空             |
| `loc` / `iloc` | 通过标签获取`DataFrame`中的一组值。 |
| `ndim`         | `DataFrame`对象的维度               |
| `shape`        | `DataFrame`对象的形状（行数和列数） |
| `size`         | `DataFrame`对象中元素的个数         |
| `values`       | `DataFrame`对象的数据对应的二维数组 |

关于`DataFrame`的方法，首先需要了解的是`info()`方法，它可帮助了解`DataFrame`的相关信息

代码：

```py
emp_df.info()
```

输出：

```
<class 'pandas.core.frame.DataFrame'>
Int64Index: 14 entries, 1359 to 7800
Data columns (total 6 columns):
 #   Column  Non-Null Count  Dtype
---  ------  --------------  -----
 0   ename   14 non-null     object
 1   job     14 non-null     object
 2   mgr     13 non-null     float64
 3   sal     14 non-null     int64
 4   comm    6 non-null      float64
 5   dno     14 non-null     int64
dtypes: float64(2), int64(2), object(2)
memory usage: 1.3+ KB
```

如果需要查看`DataFrame`的头部或尾部的数据，可使用`head()`或`tail()`方法，这两个方法的默认参数是`5`，表示获取`DataFrame`最前面 5 行或最后面 5 行的数据

```py
emp_df.head()
```

输出：

```
        ename    job    mgr    sal    comm  dno
eno
1359	胡一刀   销售员	3344   1800  200   30
2056	乔峰	   分析师	 7800   5000  1500	20
3088	李莫愁	  设计师	2056   3500  800   20
3211	张无忌	  程序员	2056   3200  NaN   20
3233	丘处机	  程序员	2056   3400	 NaN   20
```

#### 获取数据

##### 索引和切片

如果要获取`DataFrame`的某一列，例如取出上面`emp_df`的`ename`列，可使用下面的两种方式。

```py
emp_df.ename
```

或

```py
emp_df['ename']
```

执行上面的代码可发现，获得的是一个`Series`对象。事实上，`DataFrame`对象就是将多个`Series`对象组合到一起的结果。

如果要获取`DataFrame`的某一行，可使用整数索引或设置的索引，例如取出员工编号为`2056`的员工数据，代码如下所示。

```py
emp_df.iloc[1]
```

或

```py
emp_df.loc[2056]
```

通过执行上面的代码发现，单独取`DataFrame` 的某一行或某一列得到的都是`Series`对象。当然也可通过花式索引来获取多个行或多个列的数据，花式索引的结果仍是一个`DataFrame`对象。

获取多个列：

```py
emp_df[['ename', 'job']]
```

获取多个行：

```py
emp_df.loc[[2056, 7800, 3344]]
```

如果要获取或修改`DataFrame` 对象某个单元格的数据，需要同时指定行和列的索引，例如要获取员工编号为`2056`的员工的职位信息，代码如下所示。

```py
emp_df['job'][2056]
```

或

```py
emp_df.loc[2056]['job']
```

或

```py
emp_df.loc[2056, 'job']
```

推荐大家使用第三种做法，因为它只做了一次索引运算。如果要将该员工的职位修改为“架构师”，可使用下面的代码。

```py
emp_df.loc[2056, 'job'] = '架构师'
```

也可通过切片操作来获取多行多列，相信大家一定已经想到了这一点。

```py
emp_df.loc[2056:3344]
```

输出：

```
        ename    job        mgr      sal     comm    dno
eno
2056	乔峰	    分析师	    7800.0	 5000	 1500.0	 20
3088	李莫愁	   设计师	   2056.0	3500	800.0	20
3211	张无忌	   程序员	   2056.0	3200	NaN     20
3233	丘处机	   程序员	   2056.0	3400	NaN	    20
3244	欧阳锋	   程序员	   3088.0	3200	NaN     20
3251	张翠山	   程序员	   2056.0	4000	NaN	    20
3344	黄蓉	    销售主管   7800.0	3000	800.0	30
```

##### 数据筛选

上面提到了花式索引，相信大家已经联想到了布尔索引。跟`ndarray`和`Series`一样，可通过布尔索引对`DataFrame`对象进行数据筛选，例如要从`emp_df`中筛选出月薪超过`3500`的员工，代码如下所示。

```py
emp_df[emp_df.sal > 3500]
```

输出：

```
        ename    job        mgr      sal     comm    dno
eno
2056	乔峰	    分析师	    7800.0	 5000	 1500.0	 20
3251	张翠山	   程序员	   2056.0	4000	NaN	    20
5566	宋远桥	   会计师	   7800.0	4000	1000.0	10
7800	张三丰	   总裁	    NaN      9000	 1200.0	 20
```

也可组合多个条件来进行数据筛选，例如从`emp_df`中筛选出月薪超过`3500`且部门编号为`20`的员工，代码如下所示。

```py
emp_df[(emp_df.sal > 3500) & (emp_df.dno == 20)]
```

输出：

```
        ename    job        mgr      sal     comm    dno
eno
2056	乔峰	    分析师	    7800.0	 5000	 1500.0	 20
3251	张翠山	   程序员	   2056.0	4000	NaN	    20
7800	张三丰	   总裁	    NaN      9000	 1200.0	 20
```

除了使用布尔索引，`DataFrame`对象的`query`方法也可实现数据筛选，`query`方法的参数是一个字符串，它代表了筛选数据使用的表达式，而且更符合 Python 程序员的使用习惯。下面使用`query`方法将上面的效果重新实现一遍，代码如下所示。

```py
emp_df.query('sal > 3500 and dno == 20')
```

#### 重塑数据

有的时，做数据分析需要的原始数据可能并不是来自一个地方，就像上面的例子中，从关系型数据库中读取了三张表，得到了三个`DataFrame`对象，但实际工作可能需要把他们的数据整合到一起。例如：`emp_df`和`emp2_df`其实都是员工的数据，而且数据结构完全一致，可使用`pandas`提供的`concat`函数实现两个或多个`DataFrame`的数据拼接，代码如下所示。

```py
all_emp_df = pd.concat([emp_df, emp2_df])
```

输出：

```
        ename    job        mgr      sal     comm    dno
eno
1359    胡一刀    销售员	   3344.0	1800	200.0	30
2056    乔峰	    分析师	    7800.0	 5000	 1500.0	 20
3088    李莫愁	   设计师	   2056.0	3500	800.0	20
3211    张无忌	   程序员	   2056.0	3200	NaN     20
3233    丘处机	   程序员	   2056.0	3400	NaN	    20
3244    欧阳锋	   程序员	   3088.0	3200	NaN     20
3251    张翠山	   程序员	   2056.0	4000	NaN	    20
3344    黄蓉	    销售主管   7800.0	3000	800.0	30
3577    杨过	    会计	     5566.0	  2200	  NaN	  10
3588    朱九真	   会计	    5566.0	 2500	 NaN	 10
4466    苗人凤	   销售员	   3344.0	2500	NaN	    30
5234    郭靖	    出纳	     5566.0	  2000	  NaN	  10
5566    宋远桥	   会计师	   7800.0	4000	1000.0	10
7800    张三丰	   总裁	    NaN      9000	 1200.0	 20
9800    骆昊	    架构师     7800.0	 30000	 5000.0	 20
9900    王小刀	   程序员     9800.0	10000	1200.0	20
9700    王大锤	   程序员     9800.0	8000	600.0	20
```

上面的代码将两个代表员工数据的`DataFrame`拼接到了一起，接下来使用`merge`函数将员工表和部门表的数据合并到一张表中，代码如下所示。

先使用`reset_index`方法重新设置`all_emp_df`的索引，这样`eno` 不再是索引而是一个普通列，`reset_index`方法的`inplace`参数设置为`True`表示，重置索引的操作直接在`all_emp_df`上执行，而不是返回修改后的新对象。

```py
all_emp_df.reset_index(inplace=True)
```

通过`merge`函数合并数据，也可调用`DataFrame`对象的`merge`方法来达到同样的效果。

```py
pd.merge(dept_df, all_emp_df, how='inner', on='dno')
```

输出：

```
    dno dname  dloc eno   ename  job      mgr     sal    comm
0   10	会计部	北京	3577  杨过	会计	   5566.0  2200   NaN
1   10	会计部	北京	3588  朱九真  会计     5566.0  2500   NaN
2   10	会计部	北京	5234  郭靖	出纳	   5566.0  2000   NaN
3   10	会计部	北京	5566  宋远桥  会计师   7800.0	 4000   1000.0
4   20	研发部	成都	2056  乔峰	架构师   7800.0  5000	 1500.0
5   20	研发部	成都	3088  李莫愁  设计师   2056.0	 3500   800.0
6   20	研发部	成都	3211  张无忌  程序员   2056.0	 3200   NaN
7   20	研发部	成都	3233  丘处机  程序员   2056.0	 3400   NaN
8   20	研发部	成都	3244  欧阳锋  程序员   3088.0	 3200   NaN
9   20	研发部	成都	3251  张翠山  程序员   2056.0	 4000   NaN
10  20	研发部	成都	7800  张三丰  总裁     NaN     9000   1200.0
11  20	研发部	成都	9800  骆昊    架构师   7800.0  30000	 5000.0
12  20	研发部	成都	9900  王小刀  程序员	 9800.0	 10000  1200.0
13  20	研发部	成都	9700  王大锤  程序员	 9800.0	 8000   600.0
14  30	销售部	重庆	1359  胡一刀  销售员	 3344.0	 1800   200.0
15  30	销售部	重庆	3344  黄蓉    销售主管 7800.0	 3000   800.0
16  30	销售部	重庆	4466  苗人凤  销售员   3344.0	 2500   NaN
```

`merge`函数的一个参数代表合并的左表、第二个参数代表合并的右表，有 SQL 编程经验的同学对这两个词是不是感觉到非常亲切。正如大家猜想的那样，`DataFrame`对象的合并跟数据库中的表连接非常类似，所以上面代码中的`how`代表了合并两张表的方式，有`left`、`right`、`inner`、`outer`四个选项；而`on`则代表了基于哪个列实现表的合并，相当于 SQL 表连接中的连表条件，如果左右两表对应的列列名不同，可用`left_on`和`right_on`参数取代`on`参数分别进行指定。

如果对上面的代码稍作修改，将`how`参数修改为`left`，大家可思考一下代码执行的结果。

```py
pd.merge(dept_df, all_emp_df, how='left', on='dno')
```

运行结果比之前的输出多出了如下所示的一行，这是因为`left`代表左外连接，也就意味着左表`dept_df`中的数据会被完整的查出来，但是在`all_emp_df`中又没有编号为`40` 部门的员工，所以对应的位置都被填入了空值。

```
17  40  运维部  天津  NaN  NaN  NaN  NaN  NaN  NaN
```

## Pandas 的应用-3

### DataFrame 的应用

#### 数据清洗

通常，从 Excel、CSV 或数据库中获取到的数据并不是非常完美的，里面可能因为系统或人为的原因混入了重复值或异常值，也可能在某些字段上存在缺失值；再者，`DataFrame`中的数据也可能存在格式不统一、量纲不统一等各种问题。因此，在开始数据分析之前，对数据进行清洗就显得特别重要。

##### 缺失值

可使用`DataFrame`对象的`isnull`或`isna`方法来找出数据表中的缺失值

```py
emp_df.isnull()
```

或

```py
emp_df.isna()
```

输出：

```
        ename   job	    mgr     sal     comm    dno
eno
1359	False	False	False	False	False	False
2056	False	False	False	False	False	False
3088	False	False	False	False	False	False
3211	False	False	False	False	True	False
3233	False	False	False	False	True	False
3244	False	False	False	False	True	False
3251	False	False	False	False	True	False
3344	False	False	False	False	False	False
3577	False	False	False	False	True	False
3588	False	False	False	False	True	False
4466	False	False	False	False	True	False
5234	False	False	False	False	True	False
5566	False	False	False	False	False	False
7800	False	False	True	False	False	False
```

相对应的，`notnull`和`notna`方法可将非空的值标记为`True`。如果想删除这些缺失值，可使用`DataFrame`对象的`dropna`方法，该方法的`axis`参数可指定沿着 0 轴还是 1 轴删除，也就是说当遇到空值时，是删除整行还是删除整列，默认是沿 0 轴进行删除的，代码如下所示。

```py
emp_df.dropna()
```

输出：

```
        ename   job      mgr	 sal    comm     dno
eno
1359	胡一刀  销售员	3344.0	1800   200.0	30
2056	乔峰    架构师	 7800.0	 5000	1500.0	 20
3088	李莫愁  设计师	2056.0	3500   800.0	20
3344	黄蓉    销售主管	7800.0	3000   800.0	30
5566	宋远桥  会计师	7800.0	4000   1000.0	10
```

如果要沿着 1 轴进行删除，可使用下面的代码。

```py
emp_df.dropna(axis=1)
```

输出：

```
        ename    job      sal    dno
eno
1359	胡一刀   销售员    1800	30
2056	乔峰     架构师	  5000	 20
3088	李莫愁   设计师    3500	20
3211	张无忌   程序员    3200	20
3233	丘处机   程序员    3400	20
3244	欧阳锋   程序员    3200	20
3251	张翠山   程序员    4000	20
3344	黄蓉     销售主管  3000	30
3577	杨过     会计	   2200	  10
3588	朱九真   会计	  2500	 10
4466	苗人凤   销售员	 2500   30
5234	郭靖     出纳      2000   10
5566	宋远桥   会计师    4000   10
7800	张三丰   总裁      9000   20
```

> `DataFrame`对象的很多方法都有一个名为`inplace`的参数，该参数的默认值为`False`，表示的操作不会修改原来的`DataFrame`对象，而是将处理后的结果通过一个新的`DataFrame`对象返回。如果将该参数的值设置为`True`，那么的操作就会在原来的`DataFrame`上面直接修改，方法的返回值为`None`。简单的说，上面的操作并没有修改`emp_df`，而是返回了一个新的`DataFrame`对象。

在某些特定的场景下，可对空值进行填充，对应的方法是`fillna`，填充空值时可使用指定的值（通过`value`参数进行指定），也可用表格中前一个单元格（通过设置参数`method=ffill`）或后一个单元格（通过设置参数`method=bfill`）的值进行填充，当代码如下所示。

```py
emp_df.fillna(value=0)
```

> 填充的值如何选择也是一个值得探讨的话题，实际工作中，可能会使用某种统计量（如：均值、众数等）进行填充，或使用某种插值法（如：随机插值法、拉格朗日插值法等）进行填充，甚至有可能通过回归模型、贝叶斯模型等对缺失数据进行填充。

输出：

```
        ename    job        mgr      sal     comm    dno
eno
1359	胡一刀    销售员	   3344.0	1800	200.0	30
2056	乔峰	    分析师	    7800.0	 5000	 1500.0	 20
3088	李莫愁	   设计师	   2056.0	3500	800.0	20
3211	张无忌	   程序员	   2056.0	3200	0.0     20
3233	丘处机	   程序员	   2056.0	3400	0.0	    20
3244	欧阳锋	   程序员	   3088.0	3200	0.0     20
3251	张翠山	   程序员	   2056.0	4000	0.0	    20
3344	黄蓉	    销售主管   7800.0	3000	800.0	30
3577	杨过	    会计	     5566.0	  2200	  0.0	  10
3588	朱九真	   会计	    5566.0	 2500	 0.0	 10
4466	苗人凤	   销售员	   3344.0	2500	0.0	    30
5234	郭靖	    出纳	     5566.0	  2000	  0.0	  10
5566	宋远桥	   会计师	   7800.0	4000	1000.0	10
7800	张三丰	   总裁	    0.0      9000	 1200.0	 20
```

##### 重复值

先给之前的部门表添加两行数据，让部门表中名为“研发部”和“销售部”的部门各有两个。

```py
dept_df.loc[50] = {'dname': '研发部', 'dloc': '上海'}
dept_df.loc[60] = {'dname': '销售部', 'dloc': '长沙'}
dept_df
```

输出:

```
    dname  dloc
dno
10	会计部	北京
20	研发部	成都
30	销售部	重庆
40	运维部	天津
50	研发部	上海
60	销售部	长沙
```

现在，的数据表中有重复数据了，可通过`DataFrame`对象的`duplicated`方法判断是否存在重复值，该方法在不指定参数时默认判断行索引是否重复，也可指定根据部门名称`dname`判断部门是否重复，代码如下所示。

```py
dept_df.duplicated('dname')
```

输出：

```
dno
10    False
20    False
30    False
40    False
50     True
60     True
dtype: bool
```

从上面的输出可看到，`50`和`60`两个部门从部门名称上来看是重复的，如果要删除重复值，可使用`drop_duplicates`方法，该方法的`keep`参数可控制在遇到重复值时，保留第一项还是保留最后一项，或多个重复项一个都不用保留，全部删除掉。

```py
dept_df.drop_duplicates('dname')
```

输出：

```
	dname	dloc
dno
10	会计部	北京
20	研发部	成都
30	销售部	重庆
40	运维部	天津
```

将`keep`参数的值修改为`last`。

```py
dept_df.drop_duplicates('dname', keep='last')
```

输出：

```
	dname	dloc
dno
10	会计部	北京
40	运维部	天津
50	研发部	上海
60	销售部	长沙
```

##### 异常值

异常值在统计学上的全称是疑似异常值，也称作离群点（outlier），异常值的分析也称作离群点分析。异常值是指样本中出现的“极端值”，数据值看起来异常大或异常小，其分布明显偏离其余的观测值。实际工作中，有些异常值可能是由系统或人为原因造成的，但有些异常值却不是，它们能够重复且稳定的出现，属于正常的极端值，例如很多游戏产品中头部玩家的数据往往都是离群的极端值。所以，既不能忽视异常值的存在，也不能简单地把异常值从数据分析中剔除。重视异常值的出现，分析其产生的原因，常常成为发现问题进而改进决策的契机。

异常值的检测有 Z-score 方法、IQR 方法、DBScan 聚类、孤立森林等，这里对前两种方法做一个简单的介绍。

<img src="https://github.com/jackfrued/mypic/raw/master/20211004192858.png" style="zoom:50%;">

如果数据服从正态分布，依据 3σ 法则，异常值被定义与平均值的偏差超过三倍标准差的值。在正态分布下，距离平均值 3σ 之外的值出现的概率为$ P(|x-\mu|>3\sigma)<0.003 $，属于小概率事件。如果数据不服从正态分布，那么可用远离平均值的多少倍的标准差来描述，这里的倍数就是 Z-score。Z-score 以标准差为单位去度量某一原始分数偏离平均值的距离，公式如下所示。

$$
z = \frac {X - \mu} {\sigma}
$$

Z-score 需要根据经验和实际情况来决定，通常把远离标准差`3`倍距离以上的数据点视为离群点，下面的代给出了如何通过 Z-score 方法检测异常值。

```py
import numpy as np


def detect_outliers_zscore(data, threshold=3):
    avg_value = np.mean(data)
    std_value = np.std(data)
    z_score = np.abs((data - avg_value) / std_value)
    return data[z_score > threshold]
```

IQR 方法中的 IQR（Inter-Quartile Range）代表四分位距离，即上四分位数（Q3）和下四分位数（Q1）的差值。通常情况下，可认为小于 $ Q1 - 1.5 \times IQR $ 或大于 $ Q3 + 1.5 \times IQR $ 的就是异常值，而这种检测异常值的方法也是箱线图（后面会讲到）默认使用的方法。下面的代给出了如何通过 IQR 方法检测异常值。

```py
import numpy as np


def detect_outliers_iqr(data, whis=1.5):
    q1, q3 = np.quantile(data, [0.25, 0.75])
    iqr = q3 - q1
    lower, upper = q1 - whis * iqr, q3 + whis * iqr
    return data[(data < lower) | (data > upper)]
```

如果要删除异常值，可使用`DataFrame`对象的`drop`方法，该方法可根据行索引或列索引删除指定的行或列。例如认为月薪低于`2000`或高于`8000`的是员工表中的异常值，可用下面的代码删除对应的记录。

```py
emp_df.drop(emp_df[(emp_df.sal > 8000) | (emp_df.sal < 2000)].index)
```

如果要替换掉异常值，可通过给单元格赋值的方式来实现，也可使用`replace`方法将指定的值替换掉。例如要将月薪为`1800`和`9000`的替换为月薪的平均值，补贴为`800`的替换为`1000`，代码如下所示。

```py
avg_sal = np.mean(emp_df.sal).astype(int)
emp_df.replace({'sal': [1800, 9000], 'comm': 800}, {'sal': avg_sal, 'comm': 1000})
```

##### 预处理

对数据进行预处理也是一个很大的话题，它包含了对数据的拆解、变换、归约、离散化等操作。先来看看数据的拆解。如果数据表中的数据是一个时间日期，通常都需要从年、季度、月、日、星期、小时、分钟等维度对其进行拆解，如果时间日期是用字符串表示的，可先通过`pandas`的`to_datetime`函数将其处理成时间日期。

在下面的例子中，先读取 Excel 文件，获取到一组销售数据，其中第一列就是销售日期，将其拆解为“月份”、“季度”和“星期”，代码如下所示。

```py
sales_df = pd.read_excel(
    '2020年销售数据.xlsx',
    usecols=['销售日期', '销售区域', '销售渠道', '品牌', '销售额']
)
sales_df.info()
```

> 如果需要上面例子中的 Excel 文件，可通过下面的百度云盘地址进行获取，数据在《从零开始学数据分析》目录中。链接：https://pan.baidu.com/s/1rQujl5RQn9R7PadB2Z5g_g，提取码：e7b4。

输出：

```
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 1945 entries, 0 to 1944
Data columns (total 5 columns):
 #   Column  Non-Null Count  Dtype
---  ------  --------------  -----
 0   销售日期    1945 non-null   datetime64[ns]
 1   销售区域    1945 non-null   object
 2   销售渠道    1945 non-null   object
 3   品牌        1945 non-null   object
 4   销售额      1945 non-null   int64
dtypes: datetime64[ns](1), int64(1), object(3)
memory usage: 76.1+ KB
```

```py
sales_df['月份'] = sales_df['销售日期'].dt.month
sales_df['季度'] = sales_df['销售日期'].dt.quarter
sales_df['星期'] = sales_df['销售日期'].dt.weekday
sales_df
```

输出：

```
	    销售日期	 销售区域	销售渠道	品牌	  销售额	月份	季度	星期
0	    2020-01-01	上海	     拼多多	 八匹马   8217	    1	 1	   2
1	    2020-01-01	上海	     抖音	      八匹马	6351	 1	  1	    2
2	    2020-01-01	上海	     天猫	      八匹马	14365	 1	  1	    2
3	    2020-01-01	上海	     天猫       八匹马	2366	 1	  1     2
4	    2020-01-01	上海	     天猫 	  皮皮虾	15189	 1	  1     2
...     ...         ...        ...       ...      ...     ...  ...   ...
1940    2020-12-30	北京	     京东	      花花姑娘 6994     12	 4	   2
1941    2020-12-30	福建	     实体	      八匹马	7663	 12	  4	    2
1942    2020-12-31	福建	     实体	      花花姑娘 14795    12	 4	   3
1943    2020-12-31	福建	     抖音	      八匹马	3481	 12	  4	    3
1944    2020-12-31	福建	     天猫	      八匹马	2673	 12	  4	    3
```

在上面的代码中，通过日期时间类型的`Series`对象的`dt` 属性，获得一个访问日期时间的对象，通过该对象的`year`、`month`、`quarter`、`hour`等属性，就可获取到年、月、季度、小时等时间信息，获取到的仍是一个`Series`对象，它包含了一组时间信息，所以通常也将这个`dt`属性称为“日期时间向量”。

再来说一说字符串类型的数据的处理，先从指定的 Excel 文件中读取某招聘网站的招聘数据。

```py
jobs_df = pd.read_csv(
    '某招聘网站招聘数据.csv',
    usecols=['city', 'companyFullName', 'positionName', 'salary']
)
jobs_df.info()
```

> 如果需要上面例子中的 Excel 文件，可通过下面的百度云盘地址进行获取，数据在《从零开始学数据分析》目录中。链接：https://pan.baidu.com/s/1rQujl5RQn9R7PadB2Z5g_g，提取码：e7b4。

输出：

```
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 3140 entries, 0 to 3139
Data columns (total 4 columns):
 #   Column           Non-Null Count  Dtype
---  ------           --------------  -----
 0   city             3140 non-null   object
 1   companyFullName  3140 non-null   object
 2   positionName     3140 non-null   object
 3   salary           3140 non-null   object
dtypes: object(4)
memory usage: 98.2+ KB
```

查看前`5`条数据。

```py
jobs_df.head()
```

输出：

```
    city    companyFullName              positionName    salary
0   北京	  达疆网络科技（上海）有限公司    数据分析岗       15k-30k
1   北京	  北京音娱时光科技有限公司        数据分析        10k-18k
2   北京	  北京千喜鹤餐饮管理有限公司	     数据分析        20k-30k
3   北京	  吉林省海生电子商务有限公司	     数据分析        33k-50k
4   北京	  韦博网讯科技（北京）有限公司	数据分析        10k-15k
```

上面的数据表一共有`3140`条数据，但并非所有的职位都是“数据分析”的岗位，如果要筛选出数据分析的岗位，可通过检查`positionName`字段是否包含“数据分析”这个关键词，这里需要模糊匹配，应该如何实现呢？可先获取`positionName`列，因为这个`Series`对象的`dtype`是字符串，所以可通过`str`属性获取对应的字符串向量，就可利用熟悉的字符串的方法来对其进行操作，代码如下所示。

```py
jobs_df = jobs_df[jobs_df.positionName.str.contains('数据分析')]
jobs_df.shape
```

输出：

```
(1515, 4)
```

可看出，筛选后的数据还有`1515`条。还需要对`salary`字段进行处理，如果希望统计所有岗位的平均工资或每个城市的平均工资，首先需要将用范围表示的工资处理成其中间值，代码如下所示。

```py
jobs_df.salary.str.extract(r'(\d+)[kK]?-(\d+)[kK]?')
```

> 上面的代码通过正则表达式捕获组从字符串中抽取出两组数字，分别对应工资的下限和上限，对正则表达式不熟悉的读者，可阅读我的知乎专栏“从零开始学 Python”中的[《正则表达式的应用》](https://zhuanlan.zhihu.com/p/158929767)一文。

输出：

```
        0     1
0	    15    30
1	    10	  18
2       20    30
3       33    50
4       10    15
...     ...   ...
3065    8     10
3069    6     10
3070    2     4
3071    6     12
3088    8     12
```

需要提醒大家的是，抽取出来的两列数据都是字符串类型的值，需要将其转换成`int`类型，才能计算平均值，对应的方法是`DataFrame`对象的`applymap`方法，该方法的参数是一个函数，而该函数会作用于`DataFrame`中的每个元素。完成这一步之后，就可使用`apply`方法将上面的`DataFrame`处理成中间值，`apply`方法的参数也是一个函数，可通过指定`axis`参数使其作用于`DataFrame` 对象的行或列，代码如下所示。

```py
temp_df = jobs_df.salary.str.extract(r'(\d+)[kK]?-(\d+)[kK]?').applymap(int)
temp_df.apply(np.mean, axis=1)
```

输出：

```
0       22.5
1       14.0
2       25.0
3       41.5
4       12.5
        ...
3065    9.0
3069    8.0
3070    3.0
3071    9.0
3088    10.0
Length: 1515, dtype: float64
```

可用上面的结果替换掉原来的`salary`列或增加一个新的列来表示职位对应的工资，完整的代码如下所示。

```py
temp_df = jobs_df.salary.str.extract(r'(\d+)[kK]?-(\d+)[kK]?').applymap(int)
jobs_df['salary'] = temp_df.apply(np.mean, axis=1)
jobs_df.head()
```

输出：

```
    city    companyFullName              positionName    salary
0   北京	  达疆网络科技（上海）有限公司    数据分析岗       22.5
1   北京	  北京音娱时光科技有限公司        数据分析        14.0
2   北京	  北京千喜鹤餐饮管理有限公司	     数据分析        25.0
3   北京	  吉林省海生电子商务有限公司	     数据分析        41.5
4   北京	  韦博网讯科技（北京）有限公司	数据分析        12.5
```

`applymap`和`apply`两个方法在数据预处理的时经常用到，`Series`对象也有`apply`方法，也是用于数据的预处理，但是`DataFrame`对象还有一个名为`transform` 的方法，也是通过传入的函数对数据进行变换，类似`Series`对象的`map`方法。需要强调的是，`apply`方法具有归约效果的，简单的说就是能将较多的数据处理成较少的数据或一条数据；而`transform`方法没有归约效果，只能对数据进行变换，原来有多少条数据，处理后还是有多少条数据。

如果要对数据进行深度的分析和挖掘，字符串、日期时间这样的非数值类型都需要处理成数值，因为非数值类型没有办法计算相关性，也没有办法进行$\chi^2$检验等操作。对于字符串类型，通常可其分为以下三类，再进行对应的处理。

1. 有序变量（Ordinal Variable）：字符串表示的数据有顺序关系，那么可对字符串进行序号化处理。
2. 分类变量（Categorical Variable）/ 名义变量（Nominal Variable）：字符串表示的数据没有大小关系和等级之分，那么就可使用独热编码的方式处理成哑变量（虚拟变量）矩阵。
3. 定距变量（Scale Variable）：字符串本质上对应到一个有大小高低之分的数据，而且可进行加减运算，那么只需要将字符串处理成对应的数值即可。

对于第 1 类和第 3 类，可用上面提到的`apply`或`transform`方法来处理，也可利用`scikit-learn`中的`OrdinalEncoder`处理第 1 类字符串，这个在后续的课程中会讲到。对于第 2 类字符串，可使用`pandas`的`get_dummies()`函数来生成哑变量（虚拟变量）矩阵，代码如下所示。

```py
persons_df = pd.DataFrame(
    data={
        '姓名': ['关羽', '张飞', '赵云', '马超', '黄忠'],
        '职业': ['医生', '医生', '程序员', '画家', '教师'],
        '学历': ['研究生', '大专', '研究生', '高中', '本科']
    }
)
persons_df
```

输出：

```
	姓名	职业	学历
0	关羽	医生	研究生
1	张飞	医生	大专
2	赵云	程序员	研究生
3	马超	画家	高中
4	黄忠	教师	本科
```

将职业处理成哑变量矩阵。

```py
pd.get_dummies(persons_df['职业'])
```

输出：

```
    医生 教师  画家  程序员
0	1    0    0    0
1	1    0    0    0
2	0    0    0    1
3	0    0    1    0
4	0    1    0    0
```

将学历处理成大小不同的值。

```py
def handle_education(x):
    edu_dict = {'高中': 1, '大专': 3, '本科': 5, '研究生': 10}
    return edu_dict.get(x, 0)


persons_df['学历'].apply(handle_education)
```

输出：

```
0    10
1     3
2    10
3     1
4     5
Name: 学历, dtype: int64
```

再来说说数据离散化。离散化也叫分箱，如果变量的取值是连续值，那么它的取值有无数种可能，在进行数据分组的时就会非常的不方便，这个时将连续变量离散化就显得非常重要。之所以把离散化叫做分箱，是因为可预先设置一些箱子，每个箱子代表了数据取值的范围，这样就可将连续的值分配到不同的箱子中，从而实现离散化。下面的例子读取了 2018 年北京积分落户数据，可根据落户积分对数据进行分组，具体的做法如下所示。

```py
luohu_df = pd.read_csv('data/2018年北京积分落户数据.csv', index_col='id')
luohu_df.score.describe()
```

输出：

```
count    6019.000000
mean       95.654552
std         4.354445
min        90.750000
25%        92.330000
50%        94.460000
75%        97.750000
max       122.590000
Name: score, dtype: float64
```

可看出，落户积分的最大值是`122.59`，最小值是`90.75`，那么可构造一个从`90`分到`125`分，每`5`分一组的`7`个箱子，`pandas`的`cut`函数可帮助首先数据分箱，代码如下所示。

```py
bins = np.arange(90, 126, 5)
pd.cut(luohu_df.score, bins, right=False)
```

> `cut`函数的`right`参数默认值为`True`，表示箱子左开右闭；修改为`False`可让箱子的右边界为开区间，左边界为闭区间，大家看看下面的输出就明白了。

输出：

```
id
1       [120, 125)
2       [120, 125)
3       [115, 120)
4       [115, 120)
5       [115, 120)
           ...
6015      [90, 95)
6016      [90, 95)
6017      [90, 95)
6018      [90, 95)
6019      [90, 95)
Name: score, Length: 6019, dtype: category
Categories (7, interval[int64, left]): [[90, 95) < [95, 100) < [100, 105) < [105, 110) < [110, 115) < [115, 120) < [120, 125)]
```

可根据分箱的结果对数据进行分组，使用聚合函数对每个组进行统计，这是数据分析中经常用到的操作，下一个章节会为大家介绍。除此之外，`pandas`还提供了一个名为`qcut`的函数，可指定分位数对数据进行分箱，有兴趣的读者可自行研究。

## Pandas 的应用-4

### DataFrame 的应用

#### 数据分析

经过前面的学习，已经将数据准备就绪而且变成了想要的样子，接下来就是最为重要的数据分析阶段了。当拿到一大堆数据的时，如何从数据中迅速的解读出有价值的信息，这就是数据分析要解决的问题。首先，可获取数据的描述性统计信息，通过描述性统计信息，可了解数据的集中趋势和离散趋势。

例如，有如下所示的学生成绩表。

```py
import numpy as np
import pandas as pd

scores = np.random.randint(50, 101, (5, 3))
names = ('关羽', '张飞', '赵云', '马超', '黄忠')
courses = ('语文', '数学', '英语')
df = pd.DataFrame(data=scores, columns=courses, index=names)
df
```

输出：

```
     语文   数学   英语
关羽  96    72    73
张飞  72    70	97
赵云  74    51	79
马超  100   54	54
黄忠  89    100	88
```

可通过`DataFrame`对象的方法`mean`、`max`、`min`、`std`、`var`等方法分别获取每个学生或每门课程的平均分、最高分、最低分、标准差、方差等信息，也可直接通过`describe`方法直接获取描述性统计信息，代码如下所示。

计算每门课程成绩的平均分。

```py
df.mean()
```

输出：

```
语文    86.2
数学    69.4
英语    78.2
dtype: float64
```

计算每个学生成绩的平均分。

```py
df.mean(axis=1)
```

输出：

```
关羽    80.333333
张飞    79.666667
赵云    68.000000
马超    69.333333
黄忠    92.333333
dtype: float64
```

计算每门课程成绩的方差。

```py
df.var()
```

输出：

```
语文    161.2
数学    379.8
英语    265.7
dtype: float64
```

> 通过方差可看出，数学成绩波动最大，最不稳定。

获取每门课程的描述性统计信息。

```py
df.describe()
```

输出：

```
        语文        数学         英语
count   5.000000	5.000000	5.000000
mean    86.200000	69.400000	78.200000
std     12.696456	19.488458	16.300307
min     72.000000	51.000000	54.000000
25%     74.000000	54.000000	73.000000
50%     89.000000	70.000000	79.000000
75%     96.000000	72.000000	88.000000
max     100.000000	100.000000	97.000000
```

##### 排序和 Top-N

如果需要对数据进行排序，可使用`DataFrame`对象的`sort_values`方法，该方法的`by`参数可指定根据哪个列或哪些列进行排序，而`ascending`参数可指定升序或是降序。例如，下面的代码展示了如何将学生表按语文成绩排降序。

```py
df.sort_values(by='语文', ascending=False)
```

输出：

```
      语文   数学   英语
马超	100    54	  54
关羽	96     72     73
黄忠	89     100    88
赵云	74     51     79
张飞	72     70     97
```

如果`DataFrame`数据量很大，排序将是一个非常耗费时间的操作。有的时只需要获得排前 N 名或后 N 名的数据，这个时其实没有必要对整个数据进行排序，而是直接利用堆结构找出 Top-N 的数据。`DataFrame`的`nlargest`和`nsmallest`方法就提供对 Top-N 操作的支持，代码如下所示。

找出语文成绩前 3 名的学生信息。

```py
df.nlargest(3, '语文')
```

输出：

```
      语文   数学   英语
马超	100    54	  54
关羽	96     72     73
黄忠	89     100    88
```

找出数学成绩最低的 3 名学生的信息。

```py
df.nsmallest(3, '数学')
```

输出：

```
      语文  数学  英语
赵云  74    51	79
马超  100   54	54
张飞  72    70	97
```

##### 分组聚合操作

先从 Excel 文件中读取一组销售数据，再为大家演示如何进行分组聚合操作。

```py
df = pd.read_excel('2020年销售数据.xlsx')
df.head()
```

> 如果需要上面例子中的 Excel 文件，可通过百度云盘进行获取。链接：https://pan.baidu.com/s/1NhWtYcpFzF72cxcsoDoXjQ?pwd=swg1，提取码：swg1。

输出：

```
    销售日期	 销售区域   销售渠道  销售订单     品牌    售价  销售数量
0   2020-01-01  上海       拼多多    182894-455  八匹马  99    83
1   2020-01-01  上海       抖音      205635-402  八匹马  219   29
2   2020-01-01  上海       天猫      205654-021  八匹马  169   85
3   2020-01-01  上海       天猫      205654-519  八匹马  169   14
4   2020-01-01  上海       天猫      377781-010  皮皮虾  249   61
```

如果要统计每个销售区域的销售总额，可先通过“售价”和“销售数量”计算出销售额，为`DataFrame`添加一个列，代码如下所示。

```py
df['销售额'] = df['售价'] * df['销售数量']
df.head()
```

输出：

```
    销售日期	 销售区域   销售渠道  销售订单     品牌    售价  销售数量  销售额
0   2020-01-01  上海       拼多多    182894-455  八匹马  99    83        8217
1   2020-01-01  上海       抖音      205635-402  八匹马  219   29        6351
2   2020-01-01  上海       天猫      205654-021  八匹马  169   85        14365
3   2020-01-01  上海       天猫      205654-519  八匹马  169   14        2366
4   2020-01-01  上海       天猫      377781-010  皮皮虾  249   61        15189
```

再根据“销售区域”列对数据进行分组，这里使用的是`DataFrame`对象的`groupby`方法。分组之后，取“销售额”这个列在分组内进行求和处理，代码和结果如下所示。

```py
df.groupby('销售区域').销售额.sum()
```

输出：

```
销售区域
上海    11610489
北京    12477717
南京     1767301
安徽      895463
广东     1617949
江苏      537079
浙江      687862
福建    10178227
Name: 销售额, dtype: int64
```

如果要统计每个月的销售总额，可将“销售日期”作为 groupby`方法的参数，当然这里需要先将“销售日期”处理成月，代码和结果如下所示。

```py
df.groupby(df['销售日期'].dt.month).销售额.sum()
```

输出：

```
销售日期
1     5409855
2     4608455
3     4164972
4     3996770
5     3239005
6     2817936
7     3501304
8     2948189
9     2632960
10    2375385
11    2385283
12    1691973
Name: 销售额, dtype: int64
```

接下来将难度升级，统计每个销售区域每个月的销售总额，这又该如何处理呢？事实上，`groupby`方法的第一个参数可是一个列表，列表中可指定多个分组的依据，大家看看下面的代码和输出结果就明白了。

```py
df.groupby(['销售区域', df['销售日期'].dt.month]).销售额.sum()
```

输出：

```
销售区域  销售日期
上海      1       1679125
          2       1689527
          3       1061193
          4       1082187
          5        841199
          6        785404
          7        863906
          8        734937
          9       1107693
         10       412108
         11       825169
         12       528041
北京     1       1878234
         2       1807787
         3       1360666
         4       1205989
         5        807300
         6       1216432
         7       1219083
         8        645727
         9        390077
        10       671608
        11       678668
        12       596146
南京     7        841032
        10       710962
        12       215307
安徽     4        341308
         5        554155
广东     3        388180
         8        469390
         9        365191
        11       395188
江苏     4        537079
浙江     3        248354
         8        439508
福建     1       1852496
         2       1111141
         3       1106579
         4        830207
         5       1036351
         6        816100
         7        577283
         8        658627
         9        769999
        10       580707
        11       486258
        12       352479
Name: 销售额, dtype: int64
```

如果希望统计出每个区域的销售总额以及每个区域单笔金额的最高和最低，可在`DataFrame`或`Series`对象上使用`agg`方法并指定多个聚合函数，代码和结果如下所示。

```py
df.groupby('销售区域').销售额.agg(['sum', 'max', 'min'])
```

输出：

```
          sum        max        min
销售区域
上海      11610489   116303     948
北京      12477717   133411     690
南京      1767301    87527      1089
安徽      895463     68502      1683
广东      1617949    120807     990
江苏      537079     114312     3383
浙江      687862     90909      3927
福建      10178227   87527      897
```

如果希望自定义聚合后的列的名字，可使用如下所示的方法。

```py
df.groupby('销售区域').销售额.agg(销售总额='sum', 单笔最高='max', 单笔最低='min')
```

输出：

```
          销售总额   单笔最高    单笔最低
销售区域
上海      11610489   116303     948
北京      12477717   133411     690
南京      1767301    87527      1089
安徽      895463     68502      1683
广东      1617949    120807     990
江苏      537079     114312     3383
浙江      687862     90909      3927
福建      10178227   87527      897
```

如果需要对多个列使用不同的聚合函数，例如“统计每个销售区域销售额的平均值以及销售数量的最低值和最高值”，可按照下面的方式来操作。

```py
df.groupby('销售区域')[['销售额', '销售数量']].agg({
    '销售额': 'mean', '销售数量': ['max', 'min']
})
```

输出：

```
         销售额        销售数量
         mean          max    min
销售区域
上海     20622.538188  100    10
北京     20125.350000  100    10
南京     22370.898734  100    11
安徽     26337.147059  98     16
广东     32358.980000  98     10
江苏     29837.722222  98     15
浙江     27514.480000  95     20
福建     18306.163669  100    10
```

##### 透视表和交叉表

上面的例子中，“统计每个销售区域每个月的销售总额”会产生一个看起来很长的结果，在实际工作中通常把那些行很多列很少的表成为“窄表”，如果不想得到这样的一个“窄表”，可使用`DataFrame`的`pivot_table`方法或是`pivot_table`函数来生成透视表。透视表的本质就是对数据进行分组聚合操作，**根据 A 列对 B 列进行统计**，如果大家有使用 Excel 的经验，相信对透视表这个概念一定不会陌生。例如，要“统计每个销售区域的销售总额”，那么“销售区域”就是的 A 列，而“销售额”就是的 B 列，在`pivot_table`函数中分别对应`index`和`values`参数，这两个参数都可是单个列或多个列。

```py
pd.pivot_table(df, index='销售区域', values='销售额', aggfunc='sum')
```

输出：

<img src="https://github.com/jackfrued/mypic/raw/master/20211106180912.png" style="zoom:50%">

> 上面的结果操作跟之前用`groupby`的方式得到的结果有一些区别，`groupby`操作后，如果对单个列进行聚合，得到的结果是一个`Series`对象，而上面的结果是一个`DataFrame` 对象。

如果要统计每个销售区域每个月的销售总额，也可使用`pivot_table`函数，代码如下所示。

```py
pd.pivot_table(df, index=['销售区域', df['销售日期'].dt.month], values='销售额', aggfunc='sum')
```

上面的操作结果是一个`DataFrame`，但也是一个长长的“窄表”，如果希望做成一个行比较少列比较多的“宽表”，可将`index`参数中的列放到`columns`参数中，代码如下所示。

```py
pd.pivot_table(
    df, index='销售区域', columns=df['销售日期'].dt.month,
    values='销售额', aggfunc='sum', fill_value=0
)
```

> `pivot_table`函数的`fill_value=0`会将空值处理为`0`。

输出：

<img src="https://github.com/jackfrued/mypic/raw/master/20211106104551.png" style="zoom:50%">

使用`pivot_table`函数时，还可通过添加`margins`和`margins_name`参数对分组聚合的结果做一个汇总，具体的操作和效果如下所示。

```py
df['月份'] = df['销售日期'].dt.month
pd.pivot_table(
    df, index='销售区域', columns='月份',
    values='销售额', aggfunc='sum', fill_value=0,
    margins=True, margins_name='总计'
)
```

输出：

![image-20211106181707655](https://github.com/jackfrued/mypic/raw/master/20211106181707.png)

交叉表就是一种特殊的透视表，它不需要先构造一个`DataFrame`对象，而是直接通过数组或`Series`对象指定两个或多个因素进行运算得到统计结果。例如，要统计每个销售区域的销售总额，也可按照如下所示的方式来完成，先准备三组数据。

```py
sales_area, sales_month, sales_amount = df['销售区域'], df['月份'], df['销售额']
```

使用`crosstab`函数生成交叉表。

```py
pd.crosstab(
    index=sales_area, columns=sales_month, values=sales_amount, aggfunc='sum'
).fillna(0).applymap(int)
```

> 上面的代码使用了`DataFrame`对象的`fillna`方法将空值处理为 0，再使用`applymap`方法将数据类型处理成整数。

#### 数据可视化

一图胜千言，对数据进行透视的结果，最终要通过图表的方式呈现出来，因为图表具有极强的表现力，能够让迅速的解读数据中隐藏的价值。和`Series`一样，`DataFrame`对象提供了`plot`方法来支持绘图，底层仍是通过`matplotlib`库实现图表的渲染。关于`matplotlib`的内容，在下一个章节进行详细的探讨，这里只简单的讲解`plot`方法的用法。

例如，想通过一张柱状图来比较“每个销售区域的销售总额”，可直接在透视表上使用`plot`方法生成柱状图。先导入`matplotlib.pyplot`模块，通过修改绘图的参数使其支持中文显示。

```py
import matplotlib.pyplot as plt

plt.rcParams['font.sans-serif'] = 'FZJKai-Z03S'
```

> 上面的`FZJKai-Z03S`是我电脑上已经安装的一种支持中文的字体的名称，字体的名称可通过查看用户主目录下`.matplotlib`文件夹下名为`fontlist-v330.json`的文件来获得，而这个文件在执行上面的命令后就会生成。

使用魔法指令配置生成矢量图。

```py
%config InlineBackend.figure_format = 'svg'
```

绘制“每个销售区域销售总额”的柱状图。

```py
temp = pd.pivot_table(df, index='销售区域', values='销售额', aggfunc='sum')
temp.plot(figsize=(8, 4), kind='bar')
plt.xticks(rotation=0)
plt.show()
```

> 上面的第 3 行代码会将横轴刻度上的文字旋转到 0 度，第 4 行代码会显示图像。

输出：

<img src="https://github.com/jackfrued/mypic/raw/master/20211106195040.png" style="zoom:50%">

如果要绘制饼图，可修改`plot`方法的`kind`参数为`pie`，使用定制饼图的参数对图表加以定制，代码如下所示。

```py
temp.sort_values(by='销售额', ascending=False).plot(
    figsize=(6, 6), kind='pie', y='销售额',
    autopct='%.2f%%', pctdistance=0.8,
    wedgeprops=dict(linewidth=1, width=0.35)
)
plt.legend(loc='center')
plt.show()
```

输出：

<img src="https://github.com/jackfrued/mypic/raw/master/20211106201550.png" style="zoom:50%">

## Pandas 的应用-5

### DataFrame 的应用

#### 窗口计算

`DataFrame`对象的`rolling`方法允许将数据置于窗口中，就可使用函数对窗口中的数据进行运算和处理。例如，获取了某只股票近期的数据，想制作 5 日均线和 10 日均线，那么就需要先设置窗口再进行运算。可使用三方库`pandas-datareader`来获取指定的股票在某个时间段内的数据，具体的操作如下所示。

安装`pandas-datareader`三方库。

```shell
pip install pandas-datareader
```

通过`pandas-datareader` 提供的`get_data_stooq`从 Stooq 网站获取百度（股票代码：BIDU）近期股票数据。

```py
import pandas_datareader as pdr

baidu_df = pdr.get_data_stooq('BIDU', start='2021-11-22', end='2021-12-7')
baidu_df.sort_index(inplace=True)
baidu_df
```

输出：

<img src="https://github.com/jackfrued/mypic/raw/master/20211208205710.png" style="zoom:38%;">

上面的`DataFrame`有`Open`、`High`、`Low`、`Close`、`Volume`五个列，分别代码股票的开盘价、最高价、最低价、收盘价和成交量，接下来对百度的股票数据进行窗口计算。

```py
baidu_df.rolling(5).mean()
```

输出：

<img src="https://github.com/jackfrued/mypic/raw/master/20211208205932.png" style="zoom:38%;">

上面的`Close` 列的数据就是需要的 5 日均线，也可用下面的方法，直接在`Close`列对应的`Series`对象上计算 5 日均线。

```py
baidu_df.Close.rolling(5).mean()
```

输出：

```
Date
2021-11-22        NaN
2021-11-23        NaN
2021-11-24        NaN
2021-11-26        NaN
2021-11-29    150.608
2021-11-30    151.014
2021-12-01    150.682
2021-12-02    150.196
2021-12-03    147.062
2021-12-06    146.534
2021-12-07    146.544
Name: Close, dtype: float64
```

#### 相关性判定

在统计学中，通常使用协方差（covariance）来衡量两个随机变量的联合变化程度。如果变量 $X$ 的较大值主要与另一个变量 $Y$ 的较大值相对应，而两者较小值也相对应，那么两个变量倾向于表现出相似的行为，协方差为正。如果一个变量的较大值主要对应于另一个变量的较小值，则两个变量倾向于表现出相反的行为，协方差为负。简单的说，协方差的正负号显示着两个变量的相关性。方差是协方差的一种特殊情况，即变量与自身的协方差。

$$
cov(X,Y) = E((X - \mu)(Y - \upsilon)) = E(X \cdot Y) - \mu\upsilon
$$

如果 $X$ 和 $Y$ 是统计独立的，那么二者的协方差为 0，这是因为在 $X$ 和 $Y$ 独立的情况下：

$$
E(X \cdot Y) = E(X) \cdot E(Y) = \mu\upsilon
$$

协方差的数值大小取决于变量的大小，通常是不容易解释的，但是正态形式的协方差大小可显示两变量线性关系的强弱。在统计学中，皮尔逊积矩相关系数就是正态形式的协方差，它用于度量两个变量 $X$ 和 $Y$ 之间的相关程度（线性相关），其值介于`-1`到`1`之间。

$$
\rho{X,Y} = \frac {cov(X, Y)} {\sigma_{X}\sigma_{Y}}
$$

估算样本的协方差和标准差，可得到样本皮尔逊系数，通常用希腊字母 $\rho$ 表示。

$$
\rho = \frac {\sum_{i=1}^{n}(X_i - \bar{X})(Y_i - \bar{Y})} {\sqrt{\sum_{i=1}^{n}(X_i - \bar{X})^2} \sqrt{\sum_{i=1}^{n}(Y_i - \bar{Y})^2}}
$$

用 $\rho$ 值判断指标的相关性时遵循以下两个步骤。

1. 判断指标间是正相关、负相关，还是不相关。
   - 当 $ \rho \gt 0 $，认为变量之间是正相关，也就是两者的趋势一致。
   - 当 $ \rho \lt 0 $，认为变量之间是负相关，也就是两者的趋势相反。
   - 当 $ \rho = 0 $，认为变量之间是不相关的，但并不代表两个指标是统计独立的。
2. 判断指标间的相关程度。
   - 当 $ \rho $ 的绝对值在 $ [0.6,1] $ 之间，认为变量之间是强相关的。
   - 当 $ \rho $ 的绝对值在 $ [0.1,0.6) $ 之间，认为变量之间是弱相关的。
   - 当 $ \rho $ 的绝对值在 $ [0,0.1) $ 之间，认为变量之间没有相关性。

皮尔逊相关系数适用于：

1.  两个变量之间是线性关系，都是连续数据。
2.  两个变量的总体是正态分布，或接近正态的单峰分布。
3.  两个变量的观测值是成对的，每对观测值之间相互独立。

`DataFrame`对象的`cov`方法和`corr`方法分别用于计算协方差和相关系数，`corr`方法的第一个参数`method`的默认值是`pearson`，表示计算皮尔逊相关系数；除此之外，还可指定`kendall`或`spearman`来获得肯德尔系数或斯皮尔曼等级相关系数。

从名为`boston_house_price.csv`的文件中获取著名的[波士顿房价数据集](https://www.heywhale.com/mw/dataset/590bd595812ede32b73f55f2)来创建一个`DataFrame`，通过`corr`方法计算可能影响房价的`13`个因素中，哪些跟房价是正相关或负相关的，代码如下所示。

```py
boston_df = pd.read_csv('data/csv/boston_house_price.csv')
boston_df.corr()
```

> 如果需要上面例子中的 CSV 文件，可通过下面的百度云盘地址进行获取，数据在《从零开始学数据分析》目录中。链接：<https://pan.baidu.com/s/1rQujl5RQn9R7PadB2Z5g_g>，提取码：e7b4。

输出：

<img src="https://github.com/jackfrued/mypic/raw/master/20211208213325.png">

斯皮尔曼相关系数对数据条件的要求没有皮尔逊相关系数严格，只要两个变量的观测值是成对的等级评定资料，或是由连续变量观测资料转化得到的等级资料，不论两个变量的总体分布形态、样本容量的大小如何，都可用斯皮尔曼等级相关系数来进行研究。通过下面的方式来计算斯皮尔曼相关系数。

```py
boston_df.corr('spearman')
```

输出：

<img src="https://github.com/jackfrued/mypic/raw/master/20211208213518.png">

在 Notebook 或 JupyterLab 中，可为`PRICE`列添加渐变色，用颜色直观的展示出跟房价负相关、正相关、不相关的列，`DataFrame`对象`style`属性的`background_gradient`方法可完成这个操作，代码如下所示。

```py
boston_df.corr('spearman').style.background_gradient('RdYlBu', subset=['PRICE'])
```

<img src="https://github.com/jackfrued/mypic/raw/master/20211208215228.png">

上面代码中的`RdYlBu`代表的颜色如下所示，相关系数的数据值越接近`1`，颜色越接近红色；数据值越接近`1`，颜色越接近蓝色；数据值在`0`附件则是黄色。

```py
plt.get_cmap('RdYlBu')
```

<img src="https://github.com/jackfrued/mypic/raw/master/20211208215057.png">

### Index 的应用

再来看看`Index`类型，它为`Series`和`DataFrame`对象提供了索引服务，常用的`Index`有以下几种。

#### 范围索引（RangeIndex）

代码：

```py
sales_data = np.random.randint(400, 1000, 12)
month_index = pd.RangeIndex(1, 13, name='月份')
ser = pd.Series(data=sales_data, index=month_index)
ser
```

输出：

```
月份
1     703
2     705
3     557
4     943
5     961
6     615
7     788
8     985
9     921
10    951
11    874
12    609
dtype: int64
```

#### 分类索引（CategoricalIndex）

代码：

```py
cate_index = pd.CategoricalIndex(
    ['苹果', '香蕉', '苹果', '苹果', '桃子', '香蕉'],
    ordered=True,
    categories=['苹果', '香蕉', '桃子']
)
ser = pd.Series(data=amount, index=cate_index)
ser
```

输出：

```
苹果    6
香蕉    6
苹果    7
苹果    6
桃子    8
香蕉    6
dtype: int64
```

代码：

```py
ser.groupby(level=0).sum()
```

输出：

```
苹果    19
香蕉    12
桃子     8
dtype: int64
```

#### 多级索引（MultiIndex）

代码：

```py
ids = np.arange(1001, 1006)
sms = ['期中', '期末']
index = pd.MultiIndex.from_product((ids, sms), names=['学号', '学期'])
courses = ['语文', '数学', '英语']
scores = np.random.randint(60, 101, (10, 3))
df = pd.DataFrame(data=scores, columns=courses, index=index)
df
```

> 上面的代码使用了`MultiIndex`的类方法`from_product`，该方法通过`ids`和`sms`两组数据的笛卡尔积构造了多级索引。

输出：

```
             语文 数学 英语
学号	学期
1001  期中	93	77	60
      期末	93	98	84
1002  期中	64	78	71
      期末	70	71	97
1003  期中	72	88	97
      期末	99	100	63
1004  期中	80	71	61
      期末	91	62	72
1005  期中	82	95	67
      期末	84	78	86
```

代码：

```py
# 计算每个学生的成绩，期中占25%，期末占75%
df.groupby(level=0).agg(lambda x: x.values[0] * 0.25 + x.values[1] * 0.75)
```

输出：

```
        语文    数学    英语
学号
1001	93.00	92.75	78.00
1002	68.50	72.75	90.50
1003	92.25	97.00	71.50
1004	88.25	64.25	69.25
1005	83.50	82.25	81.25
```

#### 日期时间索引（DatetimeIndex）

1. 通过`date_range()`函数，可创建日期时间索引，代码如下所示。

   代码：

   ```py
   pd.date_range('2021-1-1', '2021-6-1', periods=10)
   ```

   输出：

   ```
   DatetimeIndex(['2021-01-01 00:00:00', '2021-01-17 18:40:00',
                  '2021-02-03 13:20:00', '2021-02-20 08:00:00',
                  '2021-03-09 02:40:00', '2021-03-25 21:20:00',
                  '2021-04-11 16:00:00', '2021-04-28 10:40:00',
                  '2021-05-15 05:20:00', '2021-06-01 00:00:00'],
                 dtype='datetime64[ns]', freq=None)
   ```

   代码：

   ```py
   pd.date_range('2021-1-1', '2021-6-1', freq='W')
   ```

   输出：

   ```
   DatetimeIndex(['2021-01-03', '2021-01-10', '2021-01-17', '2021-01-24',
                  '2021-01-31', '2021-02-07', '2021-02-14', '2021-02-21',
                  '2021-02-28', '2021-03-07', '2021-03-14', '2021-03-21',
                  '2021-03-28', '2021-04-04', '2021-04-11', '2021-04-18',
                  '2021-04-25', '2021-05-02', '2021-05-09', '2021-05-16',
                  '2021-05-23', '2021-05-30'],
                 dtype='datetime64[ns]', freq='W-SUN')
   ```

2. 通过`DateOffset`类型，可设置时间差并和`DatetimeIndex`进行运算，具体的操作如下所示。

   代码：

   ```py
   index = pd.date_range('2021-1-1', '2021-6-1', freq='W')
   index - pd.DateOffset(days=2)
   ```

   输出：

   ```
   DatetimeIndex(['2021-01-01', '2021-01-08', '2021-01-15', '2021-01-22',
                  '2021-01-29', '2021-02-05', '2021-02-12', '2021-02-19',
                  '2021-02-26', '2021-03-05', '2021-03-12', '2021-03-19',
                  '2021-03-26', '2021-04-02', '2021-04-09', '2021-04-16',
                  '2021-04-23', '2021-04-30', '2021-05-07', '2021-05-14',
                  '2021-05-21', '2021-05-28'],
                 dtype='datetime64[ns]', freq=None)
   ```

   代码：

   ```py
   index + pd.DateOffset(days=2)
   ```

   输出：

   ```
   DatetimeIndex(['2021-01-05', '2021-01-12', '2021-01-19', '2021-01-26',
                  '2021-02-02', '2021-02-09', '2021-02-16', '2021-02-23',
                  '2021-03-02', '2021-03-09', '2021-03-16', '2021-03-23',
                  '2021-03-30', '2021-04-06', '2021-04-13', '2021-04-20',
                  '2021-04-27', '2021-05-04', '2021-05-11', '2021-05-18',
                  '2021-05-25', '2021-06-01'],
                 dtype='datetime64[ns]', freq=None)
   ```

3. 可使用`DatatimeIndex`类型的相关方法来处理数据，具体包括：

   - `shift()`方法：通过时间前移或后移数据，仍以上面百度股票数据为例，代码如下所示。

     代码：

     ```py
     baidu_df.shift(3, fill_value=0)
     ```

     输出：

       <img src="https://github.com/jackfrued/mypic/raw/master/20211208220551.png" style="zoom:150%;">

     代码：

     ```py
     baidu_df.shift(-1, fill_value=0)
     ```

     输出：

       <img src="https://github.com/jackfrued/mypic/raw/master/20211208220713.png" style="zoom:150%;">

   - `asfreq()`方法：指定一个时间频率抽取对应的数据，代码如下所示。

     代码：

     ```py
     baidu_df.asfreq('5D')
     ```

     输出：

       <img src="https://github.com/jackfrued/mypic/raw/master/20211208221202.png">

     代码：

     ```py
     baidu_df.asfreq('5D', method='ffill')
     ```

     输出：

       <img src="https://github.com/jackfrued/mypic/raw/master/20211208221249.png" style="zoom:150%;">

   - `resample()`方法：基于时间对数据进行重采样，相当于根据时间周期对数据进行了分组操作，代码如下所示。

     代码：

     ```py
     baidu_df.resample('1M').mean()
     ```

     输出：

       <img src="https://github.com/jackfrued/mypic/raw/master/20211208221429.png">

   > 上面的代码中，`W`表示一周，`5D`表示`5`天，`1M`表示`1`个月。

4. 时区转换

   - 获取时区信息。

     ```py
     import pytz

     pytz.common_timezones
     ```

   - `tz_localize()`方法：将日期时间本地化。

     代码：

     ```py
     baidu_df = baidu_df.tz_localize('Asia/Chongqing')
     baidu_df
     ```

     输出：

       <img src="https://github.com/jackfrued/mypic/raw/master/20211208221947.png">

   - `tz_convert()`方法：转换时区。

     代码：

     ```py
     baidu_df.tz_convert('America/New_York')
     ```

     输出：

       <img src="https://github.com/jackfrued/mypic/raw/master/20211208222404.png">
